
--Creating  RDD from csv

import org.apache.spark._

val input1 = sc.textFile("data/trips/*")
val input2 = sc.textFile("data/stations/*")


  // split / clean data
  val headerAndRows = input1.map(line => line.split(",").map(_.trim))
  // get header
  val header = headerAndRows.first
  // filter out header (eh. just check if the first val matches the first header name)
  val data = headerAndRows.filter(_(0) != header(0))
  // splits to map (header/value pairs)
  val maps = data.map(splits => header.zip(splits).toMap)
  // filter out the user "me"
  //val result = maps.filter(map => map("user") != "me")
  // print result
  maps.take(2).foreach(println)
